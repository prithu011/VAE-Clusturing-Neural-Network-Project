{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95f9b90a",
   "metadata": {},
   "source": [
    "# **Feature Extraction from Audio using Variational Autoencoder (VAE)**\n",
    "\n",
    "This section describes the process of feature extraction from music data using a Variational Autoencoder (VAE). The workflow includes loading audio files, extracting Mel-spectrograms, training a VAE, and saving the latent representations for downstream tasks like clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38dfd6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "030ef301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3554 audio files\n"
     ]
    }
   ],
   "source": [
    "# Load audio files from the specified directory\n",
    "audio_dir = Path(\"../data/audio\")\n",
    "audio_files = sorted(audio_dir.glob(\"*/*.mp3\"))\n",
    "\n",
    "print(f\"Loaded {len(audio_files)} audio files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad743e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Mel-spectrogram features\n",
    "def load_audio(path, target_sr=22050):\n",
    "    audio, sr = sf.read(path, dtype='float32')\n",
    "\n",
    "    # Convert stereo to mono\n",
    "    if audio.ndim > 1:\n",
    "        audio = audio.mean(axis=1)\n",
    "\n",
    "    # Resample if needed (keyword arguments!)\n",
    "    if sr != target_sr:\n",
    "        audio = librosa.resample(\n",
    "            y=audio,\n",
    "            orig_sr=sr,\n",
    "            target_sr=target_sr\n",
    "        )\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a87d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Mel-spectrogram features\n",
    "def extract_mel(path, n_mels=64, fixed_len=1304):  \n",
    "    y, sr = librosa.load(path, sr=22050)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=512)\n",
    "    mel = np.log(mel + 1e-9)\n",
    "    \n",
    "    # Normalize to [0,1]\n",
    "    mel = (mel - mel.min()) / (mel.max() - mel.min() + 1e-9)\n",
    "    \n",
    "    # Pad or truncate to fixed_len\n",
    "    if mel.shape[1] < fixed_len:\n",
    "        pad_width = fixed_len - mel.shape[1]\n",
    "        mel = np.pad(mel, ((0,0), (0,pad_width)), mode='constant')\n",
    "    else:\n",
    "        mel = mel[:, :fixed_len]\n",
    "\n",
    "    return torch.tensor(mel, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9238d344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1304])\n"
     ]
    }
   ],
   "source": [
    "# Test the extract_mel function\n",
    "x = extract_mel(audio_files[0])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81fd25ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Custom Class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, fixed_len=1304):\n",
    "        self.audio_files = audio_files\n",
    "        self.fixed_len = fixed_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = extract_mel(self.audio_files[idx], fixed_len=self.fixed_len)\n",
    "        return mel.unsqueeze(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c60cc48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Model Definition\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=32, fixed_len=1304, n_mels=64):\n",
    "        super().__init__()\n",
    "        self.n_mels = n_mels\n",
    "        self.fixed_len = fixed_len\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Compute encoded shape dynamically\n",
    "        dummy_input = torch.zeros(1, 1, n_mels, fixed_len)\n",
    "        h = self.encoder(dummy_input)\n",
    "        self.enc_shape = h.shape[1:]  # (C,H,W)\n",
    "        self.flattened_size = h.numel() // h.shape[0]\n",
    "\n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(self.flattened_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.flattened_size, latent_dim)\n",
    "        self.fc_decode = nn.Linear(latent_dim, self.flattened_size)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=(0,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, padding=1, output_padding=(0,1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, 3, stride=2, padding=1, output_padding=(0,1)),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.fc_decode(z)\n",
    "        h = h.view(h.size(0), *self.enc_shape)\n",
    "        recon = self.decoder(h)\n",
    "        # Ensure output matches input size\n",
    "        recon = torch.nn.functional.interpolate(\n",
    "            recon, size=(self.n_mels, self.fixed_len), mode='bilinear', align_corners=False\n",
    "        )\n",
    "        return recon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8635b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE Loss Function\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = nn.functional.mse_loss(recon_x, x, reduction='mean')  \n",
    "    kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ea76a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "def train_vae(audio_files, epochs=10, batch_size=8, latent_dim=32, lr=1e-3, fixed_len=1304):\n",
    "    dataset = AudioDataset(audio_files, fixed_len=fixed_len)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = VAE(latent_dim=latent_dim, fixed_len=fixed_len).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(batch)\n",
    "            loss = vae_loss(recon, batch, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(loader):.6f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eff321ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.029492\n",
      "Epoch [2/10], Loss: 0.017347\n",
      "Epoch [3/10], Loss: 0.017001\n",
      "Epoch [4/10], Loss: 0.016912\n",
      "Epoch [5/10], Loss: 0.016804\n",
      "Epoch [6/10], Loss: 0.016812\n",
      "Epoch [7/10], Loss: 0.016667\n",
      "Epoch [8/10], Loss: 0.016703\n",
      "Epoch [9/10], Loss: 0.016638\n",
      "Epoch [10/10], Loss: 0.016648\n",
      "VAE model saved successfully at '../results/models/audio_vae.pth'\n"
     ]
    }
   ],
   "source": [
    "# Train the VAE model\n",
    "vae_model = train_vae(audio_files, epochs=10, batch_size=8, latent_dim=32, fixed_len=1304)\n",
    "os.makedirs(\"../results/models\", exist_ok=True)\n",
    "torch.save(vae_model.state_dict(), \"../results/models/audio_vae.pth\")\n",
    "print(\"VAE model saved successfully at '../results/models/audio_vae.pth'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
