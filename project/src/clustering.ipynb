{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "337d64c8",
   "metadata": {},
   "source": [
    "# **Audio Latent Space Clustering and Visualization**\n",
    "\n",
    "This project demonstrates how to perform **K-Means clustering** on latent features extracted from a **Variational Autoencoder (VAE)** trained on audio data. It also visualizes the clusters in 2D using **t-SNE**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08ebdd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3abd856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mel-spectrogram extraction\n",
    "def extract_mel(path, n_mels=64, fixed_len=1304):\n",
    "    y, sr = librosa.load(path, sr=22050)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=512)\n",
    "    mel = np.log(mel + 1e-9)\n",
    "    mel = (mel - mel.min()) / (mel.max() - mel.min() + 1e-9)\n",
    "    if mel.shape[1] < fixed_len:\n",
    "        pad_width = fixed_len - mel.shape[1]\n",
    "        mel = np.pad(mel, ((0,0),(0,pad_width)), mode='constant')\n",
    "    else:\n",
    "        mel = mel[:, :fixed_len]\n",
    "    return torch.tensor(mel, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a19f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, fixed_len=1304):\n",
    "        self.audio_files = audio_files\n",
    "        self.fixed_len = fixed_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mel = extract_mel(self.audio_files[idx], fixed_len=self.fixed_len)\n",
    "        return mel.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec550b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE model (encoder only)\n",
    "class VAE(torch.nn.Module):\n",
    "    def __init__(self, latent_dim=32, fixed_len=1304, n_mels=64):\n",
    "        super().__init__()\n",
    "        self.n_mels = n_mels\n",
    "        self.fixed_len = fixed_len\n",
    "\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 16, 3, stride=2, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, 3, stride=2, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        dummy_input = torch.zeros(1, 1, n_mels, fixed_len)\n",
    "        h = self.encoder(dummy_input)\n",
    "        self.enc_shape = h.shape[1:]\n",
    "        self.flattened_size = h.numel() // h.shape[0]\n",
    "\n",
    "        self.fc_mu = torch.nn.Linear(self.flattened_size, latent_dim)\n",
    "        self.fc_logvar = torch.nn.Linear(self.flattened_size, latent_dim)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fef67141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=83456, out_features=32, bias=True)\n",
       "  (fc_logvar): Linear(in_features=83456, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained VAE\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "latent_dim = 32\n",
    "fixed_len = 1304\n",
    "\n",
    "vae_model = VAE(latent_dim=latent_dim, fixed_len=fixed_len).to(device)\n",
    "\n",
    "# Load state dict and ignore extra keys\n",
    "state_dict = torch.load(r\"E:\\CSE425_Project\\project\\results\\models\\audio_vae.pth\", map_location=device)\n",
    "filtered_dict = {k: v for k, v in state_dict.items() if k in vae_model.state_dict()}\n",
    "vae_model.load_state_dict(filtered_dict, strict=False)\n",
    "vae_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f3aad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audio files\n",
    "audio_dir = Path(\"../data/audio\")\n",
    "audio_files = sorted(audio_dir.glob(\"*/*.mp3\"))\n",
    "dataset = AudioDataset(audio_files, fixed_len=fixed_len)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aedf3601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent features shape: (3554, 32)\n"
     ]
    }
   ],
   "source": [
    "# Extract latent features\n",
    "latent_features = []\n",
    "file_paths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(loader):\n",
    "        batch = batch.to(device)\n",
    "        mu, logvar = vae_model.encode(batch)\n",
    "        latent_features.append(mu.cpu().numpy())\n",
    "        file_paths.extend(audio_files[batch_idx*loader.batch_size : batch_idx*loader.batch_size + batch.size(0)])\n",
    "\n",
    "latent_features = np.vstack(latent_features)\n",
    "print(\"Latent features shape:\", latent_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "359228a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters assigned.\n"
     ]
    }
   ],
   "source": [
    "# K-Means clustering\n",
    "num_clusters = 10\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(latent_features)\n",
    "print(\"Clusters assigned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92a3b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results folder\n",
    "output_dir = Path(\"../results/latent_visualization\")\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c1d7fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-SNE plot saved at: ..\\results\\latent_visualization\\tsne_clusters.png\n"
     ]
    }
   ],
   "source": [
    "# t-SNE Visualization and save\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_2d = tsne.fit_transform(latent_features)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for c in range(num_clusters):\n",
    "    idxs = cluster_labels == c\n",
    "    plt.scatter(tsne_2d[idxs,0], tsne_2d[idxs,1], label=f'Cluster {c}', alpha=0.7)\n",
    "plt.title(\"t-SNE of Latent Features\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / \"tsne_clusters.png\")\n",
    "plt.close()\n",
    "print(\"t-SNE plot saved at:\", output_dir / \"tsne_clusters.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
