{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aa8a3bd",
   "metadata": {},
   "source": [
    "# **Convolutional VAE for Audio Features**\n",
    "We enhanced the previous Variational Autoencoder (VAE) by using a convolutional architecture specifically designed for 2D audio representations, such as Mel-spectrograms or MFCCs.\n",
    "Convolutional layers can efficiently capture local time-frequency patterns in audio, leading to a more expressive and compact latent representation suitable for downstream tasks like clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbd27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import glob\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e855fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ea58489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3554 audio files\n"
     ]
    }
   ],
   "source": [
    "# Load audio files from the specified directory\n",
    "audio_dir = Path(\"../data/audio\")\n",
    "audio_files = sorted(audio_dir.glob(\"*/*.mp3\"))\n",
    "\n",
    "print(f\"Loaded {len(audio_files)} audio files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "421866e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Mel-spectrogram features\n",
    "def load_audio(path, target_sr=22050):\n",
    "    audio, sr = sf.read(path, dtype='float32')\n",
    "\n",
    "    # Convert stereo to mono\n",
    "    if audio.ndim > 1:\n",
    "        audio = audio.mean(axis=1)\n",
    "\n",
    "    # Resample if needed (keyword arguments!)\n",
    "    if sr != target_sr:\n",
    "        audio = librosa.resample(\n",
    "            y=audio,\n",
    "            orig_sr=sr,\n",
    "            target_sr=target_sr\n",
    "        )\n",
    "\n",
    "    return audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e7bcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Mel-spectrogram features\n",
    "def extract_mel(path, n_mels=64, fixed_len=1304):  \n",
    "    y, sr = librosa.load(path, sr=22050)\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, hop_length=512)\n",
    "    mel = np.log(mel + 1e-9)\n",
    "    \n",
    "    # Normalize to [0,1]\n",
    "    mel = (mel - mel.min()) / (mel.max() - mel.min() + 1e-9)\n",
    "    \n",
    "    # Pad or truncate to fixed_len\n",
    "    if mel.shape[1] < fixed_len:\n",
    "        pad_width = fixed_len - mel.shape[1]\n",
    "        mel = np.pad(mel, ((0,0), (0,pad_width)), mode='constant')\n",
    "    else:\n",
    "        mel = mel[:, :fixed_len]\n",
    "\n",
    "    return torch.tensor(mel, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6a67eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1304])\n"
     ]
    }
   ],
   "source": [
    "# Test the extract_mel function\n",
    "x = extract_mel(audio_files[0])\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7f4eec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom Dataset for audio files\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_files, n_mels=64, fixed_len=1300):\n",
    "        self.audio_files = audio_files\n",
    "        self.n_mels = n_mels\n",
    "        self.fixed_len = fixed_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load audio\n",
    "        y, sr = librosa.load(self.audio_files[idx], sr=22050)\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=self.n_mels, hop_length=512)\n",
    "        mel = np.log(mel + 1e-9)\n",
    "        # Pad or truncate\n",
    "        if mel.shape[1] < self.fixed_len:\n",
    "            pad_width = self.fixed_len - mel.shape[1]\n",
    "            mel = np.pad(mel, ((0,0),(0,pad_width)), mode='constant')\n",
    "        else:\n",
    "            mel = mel[:, :self.fixed_len]\n",
    "        mel = torch.tensor(mel, dtype=torch.float32).unsqueeze(0)  \n",
    "        return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17cb9801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ConvVAE model\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, n_mels=64, fixed_len=1300, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.n_mels = n_mels\n",
    "        self.fixed_len = fixed_len\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1,16,3,stride=2,padding=1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.Conv2d(16,32,3,stride=2,padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.Conv2d(32,64,3,stride=2,padding=1), nn.BatchNorm2d(64), nn.ReLU()\n",
    "        )\n",
    "        dummy_input = torch.zeros(1,1,n_mels,fixed_len)\n",
    "        h = self.encoder(dummy_input)\n",
    "        self.enc_shape = h.shape[1:]\n",
    "        self.flattened_size = h.numel() // h.shape[0]\n",
    "\n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(self.flattened_size, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(self.flattened_size, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(latent_dim, self.flattened_size)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64,32,3,stride=2,padding=1,output_padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32,16,3,stride=2,padding=1,output_padding=1), nn.BatchNorm2d(16), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16,1,3,stride=2,padding=1,output_padding=1), nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self,x):\n",
    "        h = self.encoder(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        return self.fc_mu(h), self.fc_logvar(h)\n",
    "\n",
    "    def reparameterize(self,mu,logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self,z):\n",
    "        h = self.fc_decode(z)\n",
    "        h = h.view(-1, *self.enc_shape)\n",
    "        recon = self.decoder(h)\n",
    "        recon = F.interpolate(recon, size=(self.n_mels,self.fixed_len), mode='bilinear', align_corners=False)\n",
    "        return recon\n",
    "\n",
    "    def forward(self,x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69dfeb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE loss function\n",
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28421dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for ConvVAE\n",
    "def train_vae(audio_files, epochs=10, batch_size=8, latent_dim=32, lr=1e-3, n_mels=64, fixed_len=1300):\n",
    "    dataset = AudioDataset(audio_files, n_mels=n_mels, fixed_len=fixed_len)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    model = ConvVAE(n_mels=n_mels, fixed_len=fixed_len, latent_dim=latent_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon, mu, logvar = model(batch)\n",
    "            loss = vae_loss(recon, batch, mu, logvar)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(loader):.6f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "004c8caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8142145"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "model = ConvVAE(n_mels=64, fixed_len=1300, latent_dim=32)\n",
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb9be2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for ConvVAE \n",
    "model = ConvVAE(n_mels=64, fixed_len=1300, latent_dim=32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "torch.save(model.state_dict(), \"../results/models/conv_audio_vae.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01f43c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio latent vectors shape: (3554, 32)\n"
     ]
    }
   ],
   "source": [
    "# Extract latent vectors from trained ConvVAE\n",
    "model.eval()\n",
    "latent_vectors = []\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = AudioDataset(audio_files, n_mels=64, fixed_len=1304)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Extract latent vectors\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)                 \n",
    "        mu, logvar = model.encode(batch)    #\n",
    "        z = model.reparameterize(mu, logvar)  \n",
    "        latent_vectors.append(z.cpu().numpy()) \n",
    "\n",
    "# Convert list to numpy array\n",
    "z_audio = np.vstack(latent_vectors)\n",
    "print(\"Audio latent vectors shape:\", z_audio.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0020354d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_audio saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save latent vectors\n",
    "np.save(\"../results/z_audio.npy\", z_audio)\n",
    "print(\"z_audio saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
